{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyts.image import MarkovTransitionField, GramianAngularField\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('candles.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(n, data):\n",
    "    \n",
    "    total_prices = np.zeros((1, n))\n",
    "    for i in tqdm(range(0, len(data), n*10)):\n",
    "        d_close = data[i:i+n, 3]\n",
    "        if i + n <= len(data):\n",
    "            total_prices = np.concatenate((total_prices, d_close.reshape(1, n)), axis=0)\n",
    "    total_prices = total_prices[1:]\n",
    "    np.random.shuffle(total_prices)\n",
    "    return total_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7972/7972 [00:04<00:00, 1926.45it/s] \n"
     ]
    }
   ],
   "source": [
    "total_prices = data_gen(100, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7972, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_prices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasf = GramianAngularField(image_size=100, method='difference', sample_range=(0,1), overlapping=True)\n",
    "total_prices1 = gasf.fit_transform(total_prices).reshape(-1, 100, 100, 1)\n",
    "gasf = GramianAngularField(image_size=100, method='summation', sample_range=(0,1), overlapping=True)\n",
    "total_prices2 = gasf.fit_transform(total_prices).reshape(-1, 100, 100, 1)\n",
    "X = np.concatenate([total_prices1, total_prices2], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "tensorflow.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "nums = np.arange(len(X))\n",
    "np.random.shuffle(nums)\n",
    "X = X[nums]\n",
    "total_prices = total_prices[nums]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = X[:int(0.8*len(X))]\n",
    "test_data = X[int(0.8*len(X)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "\n",
    "input_data = tensorflow.keras.layers.Input(shape=(100, 100, 2))\n",
    "\n",
    "encoder = tensorflow.keras.layers.Conv2D(filters=512, kernel_size=5, activation='relu')(input_data)\n",
    "encoder = tensorflow.keras.layers.MaxPooling2D(2)(encoder)\n",
    "encoder = tensorflow.keras.layers.Conv2D(filters=256, kernel_size=3, activation='relu')(encoder)\n",
    "encoder = tensorflow.keras.layers.MaxPooling2D(2)(encoder)\n",
    "encoder = tensorflow.keras.layers.Conv2D(filters=128, kernel_size=3, activation='relu')(encoder)\n",
    "encoder = tensorflow.keras.layers.MaxPooling2D(2)(encoder)\n",
    "encoder = tensorflow.keras.layers.Flatten()(encoder)\n",
    "encoder = tensorflow.keras.layers.Dense(64)(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_latent_features(distribution):\n",
    "    distribution_mean, distribution_variance = distribution\n",
    "    batch_size = tensorflow.shape(distribution_variance)[0]\n",
    "    random = tensorflow.keras.backend.random_normal(shape=(batch_size, tensorflow.shape(distribution_variance)[1]))\n",
    "    return distribution_mean + tensorflow.exp(0.5 * distribution_variance) * random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_mean = tensorflow.keras.layers.Dense(64, name='mean')(encoder)\n",
    "distribution_variance = tensorflow.keras.layers.Dense(64, name='log_variance')(encoder)\n",
    "latent_encoding = tensorflow.keras.layers.Lambda(sample_latent_features)([distribution_mean, distribution_variance])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 100, 100, 2  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 96, 96, 512)  26112       ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 48, 48, 512)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 46, 46, 256)  1179904     ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 23, 23, 256)  0          ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 21, 21, 128)  295040      ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 10, 10, 128)  0          ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 12800)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           819264      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " mean (Dense)                   (None, 64)           4160        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " log_variance (Dense)           (None, 64)           4160        ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 64)           0           ['mean[0][0]',                   \n",
      "                                                                  'log_variance[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,328,640\n",
      "Trainable params: 2,328,640\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = tensorflow.keras.Model(input_data, latent_encoding)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tensorflow.keras.layers.Input(shape=(64))\n",
    "decoder = tensorflow.keras.layers.Dense(10*10*128, activation='relu')(decoder_input)\n",
    "decoder = tensorflow.keras.layers.Reshape((10, 10, 128))(decoder)\n",
    "decoder = tensorflow.keras.layers.UpSampling2D(2)(decoder)\n",
    "decoder = tensorflow.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, activation='relu')(decoder)\n",
    "decoder = tensorflow.keras.layers.UpSampling2D(2)(decoder)\n",
    "decoder = tensorflow.keras.layers.Conv2DTranspose(filters=256, kernel_size=3, activation='relu')(decoder)\n",
    "decoder = tensorflow.keras.layers.UpSampling2D(2)(decoder)\n",
    "decoder = tensorflow.keras.layers.Conv2DTranspose(filters=512, kernel_size=3, activation='relu')(decoder)\n",
    "decoder_output = tensorflow.keras.layers.Conv2DTranspose(filters=2, kernel_size=7, activation='sigmoid')(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 64)]              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 12800)             832000    \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 10, 10, 128)       0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 20, 20, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  (None, 22, 22, 128)      147584    \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 44, 44, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  (None, 46, 46, 256)      295168    \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 92, 92, 256)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_transpose_2 (Conv2DT  (None, 94, 94, 512)      1180160   \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_transpose_3 (Conv2DT  (None, 100, 100, 2)      50178     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,505,090\n",
      "Trainable params: 2,505,090\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model = tensorflow.keras.Model(decoder_input, decoder_output)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = encoder_model(input_data)\n",
    "decoded = decoder_model(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = tensorflow.keras.models.Model(input_data, decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(distribution_mean, distribution_variance):\n",
    "    \n",
    "    def get_reconstruction_loss(y_true, y_pred):\n",
    "        reconstruction_loss = tensorflow.keras.losses.mse(y_true, y_pred)\n",
    "        reconstruction_loss_batch = tensorflow.reduce_mean(reconstruction_loss)\n",
    "        return reconstruction_loss_batch*100*1\n",
    "    \n",
    "    def get_kl_loss(distribution_mean, distribution_variance):\n",
    "        kl_loss = 1 + distribution_variance - tensorflow.square(distribution_mean) - tensorflow.exp(distribution_variance)\n",
    "        kl_loss_batch = tensorflow.reduce_mean(kl_loss)\n",
    "        return kl_loss_batch*(-0.5)\n",
    "    \n",
    "    def total_loss(y_true, y_pred):\n",
    "        reconstruction_loss_batch = get_reconstruction_loss(y_true, y_pred)\n",
    "        kl_loss_batch = get_kl_loss(distribution_mean, distribution_variance)\n",
    "        return reconstruction_loss_batch + kl_loss_batch\n",
    "    \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100, 100, 2)]     0         \n",
      "                                                                 \n",
      " model (Functional)          (None, 64)                2328640   \n",
      "                                                                 \n",
      " model_1 (Functional)        (None, 100, 100, 2)       2505090   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,833,730\n",
      "Trainable params: 4,833,730\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder.compile(loss=get_loss(distribution_mean, distribution_variance), optimizer=tensorflow.keras.optimizers.Adam(lr=0.0001))\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6377 samples, validate on 1595 samples\n",
      "6377/6377 [==============================] - 1284s 201ms/sample - loss: 33.4576 - val_loss: 25.6759 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60210205c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(train_data, train_data, epochs=1, batch_size=128, validation_data=(test_data, test_data), callbacks=[ReduceLROnPlateau(patience=20, min_lr=0.00001), EarlyStopping(patience=40)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('autoencoder_RAW.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.save('encoder.hdf5')\n",
    "decoder_model.save('decoder.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusteringLayer(tensorflow.keras.layers.Layer):\n",
    "    '''\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = tensorflow.keras.layers.InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = tensorflow.keras.layers.InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(name='clusters', shape=(self.n_clusters, input_dim), initializer='glorot_uniform') \n",
    "        \n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        ''' \n",
    "        student t-distribution, as used in t-SNE algorithm.\n",
    "        It measures the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "       \n",
    "        inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        \n",
    "        Return: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        '''\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure all of the values of each sample sum up to 1.\n",
    "        \n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(2, name='clustering')(encoder_model.output)\n",
    "model = Model(inputs=encoder_model.input, outputs=clustering_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tensorflow.keras.optimizers.Adam(0.0001), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, n_init=20)\n",
    "y_pred = kmeans.fit_predict(encoder_model.predict(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "maxiter = 1000\n",
    "update_interval = 100\n",
    "index_array = np.arange(train_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tol = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [24:31<00:00,  1.47s/it] \n"
     ]
    }
   ],
   "source": [
    "for ite in tqdm(range(int(maxiter))):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(train_data, verbose=0)\n",
    "        p = target_distribution(q)\n",
    "\n",
    "    idx = index_array[index * 32: min((index+1) * 32, train_data.shape[0])]\n",
    "    loss = model.train_on_batch(x=train_data[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * 32 <= train_data.shape[0] else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = model.predict(test_data, verbose=0)\n",
    "p = target_distribution(q)\n",
    "y_pred = q.argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('label_MTF.npy',y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "s_scores = []\n",
    "d_scores = []\n",
    "for i in range(2, 9):\n",
    "    autoencoder.load_weights('autoencoder_RAW.hdf5')\n",
    "    clustering_layer = ClusteringLayer(i, name='clustering')(encoder_model.output)\n",
    "    model = Model(inputs=encoder_model.input, outputs=clustering_layer)\n",
    "    model.compile(optimizer=tensorflow.keras.optimizers.Adam(0.0001), loss='kld')\n",
    "    kmeans = KMeans(n_clusters=i, n_init=20)\n",
    "    y_pred = kmeans.fit_predict(encoder_model.predict(train_data))\n",
    "    model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "    loss = 0\n",
    "    index = 0\n",
    "    maxiter = 1000\n",
    "    update_interval = 100\n",
    "    index_array = np.arange(train_data.shape[0])\n",
    "    tol = 0.001\n",
    "    for ite in tqdm(range(int(maxiter))):\n",
    "        if ite % update_interval == 0:\n",
    "            q = model.predict(train_data, verbose=0)\n",
    "            p = target_distribution(q)\n",
    "\n",
    "        idx = index_array[index * 32: min((index+1) * 32, train_data.shape[0])]\n",
    "        loss = model.train_on_batch(x=train_data[idx], y=p[idx])\n",
    "        index = index + 1 if (index + 1) * 32 <= train_data.shape[0] else 0\n",
    "    latent = encoder_model.predict(test_data)\n",
    "    model_output = model.predict(test_data)\n",
    "    labels = []\n",
    "    for row in model_output:\n",
    "        labels.append(np.argmax(row))\n",
    "    s_scores.append(silhouette_score(latent,labels))\n",
    "    d_scores.append(davies_bouldin_score(latent, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
